<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>RAG Chatbot with LLM</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            line-height: 1.6;
            margin: 40px;
            background-color: #f8f9fa;
            color: #212529;
        }
        h1, h2, h3 {
            color: #0d6efd;
        }
        code, pre {
            background-color: #e9ecef;
            padding: 8px;
            border-radius: 5px;
            display: block;
            overflow-x: auto;
        }
        ul {
            margin-left: 20px;
        }
        .section {
            margin-bottom: 30px;
        }
        .footer {
            margin-top: 40px;
            font-size: 0.9em;
            color: #555;
        }
    </style>
</head>
<body>

<h1>ğŸ“š RAG Chatbot with LLM </h1>

<div class="section">
    <p>
        A <strong>Retrieval-Augmented Generation (RAG) chatbot</strong> built using
        <strong>Streamlit</strong>, <strong>ChromaDB</strong>, and a <strong>local LLM (Llama.cpp)</strong>.
        This application allows users to upload Markdown documents, index them into a vector database,
        and ask questions grounded in the uploaded content.
    </p>
</div>

<div class="section">
    <h2>ğŸš€ Features</h2>
    <ul>
        <li>Local LLM inference using Llama.cpp</li>
        <li>Vector similarity search with ChromaDB</li>
        <li>Upload and index Markdown documents</li>
        <li>Context-aware question answering (RAG)</li>
        <li>Multiple context synthesis strategies</li>
        <li>Chat history management</li>
        <li>Streaming token responses</li>
        <li>Configurable chunk size and overlap</li>
        <li>Persistent vector database storage</li>
    </ul>
</div>

<div class="section">
    <h2>ğŸ§  Architecture Overview</h2>
    <pre>
User Query
   â†“
Question Refinement (LLM)
   â†“
Vector Search (ChromaDB)
   â†“
Context Synthesis Strategy
   â†“
LLM Answer Generation
   â†“
Streamlit UI
    </pre>
</div>

<div class="section">
    <h2>ğŸ—‚ï¸ Project Structure</h2>
    <pre>
rag_chatbot/
â”‚
â”œâ”€â”€ bot/
â”‚   â”œâ”€â”€ client/               # LLM client (Llama.cpp)
â”‚   â”œâ”€â”€ conversation/         # Chat history & context handling
â”‚   â”œâ”€â”€ memory/               # Embedding + vector DB logic
â”‚   â”œâ”€â”€ model/                # Model registry
â”‚
â”œâ”€â”€ document_loader/
â”‚   â”œâ”€â”€ format.py
â”‚   â”œâ”€â”€ text_splitter.py
â”‚
â”œâ”€â”€ entities/
â”‚   â””â”€â”€ document.py
â”‚
â”œâ”€â”€ helpers/
â”‚   â”œâ”€â”€ log.py
â”‚   â””â”€â”€ prettier.py
â”‚
â”œâ”€â”€ models/                   # Local LLM models
â”œâ”€â”€ vector_store/             # Persistent ChromaDB index
â”œâ”€â”€ images/
â”‚   â””â”€â”€ bot.png
â”‚
â”œâ”€â”€ rag_chatbot_app.py
â””â”€â”€ README.html
    </pre>
</div>

<div class="section">
    <h2>âš™ï¸ Installation</h2>
    <h3>1. Clone Repository</h3>
    <pre>
git clone https://github.com/your-username/rag-chatbot.git
cd rag-chatbot
    </pre>

    <h3>2. Create Virtual Environment</h3>
    <pre>
python -m venv venv
source venv/bin/activate   (Linux/Mac)
venv\Scripts\activate      (Windows)
    </pre>

    <h3>3. Install Dependencies</h3>
    <pre>
pip install -r requirements.txt
    </pre>
</div>

<div class="section">
    <h2>ğŸ¤– Model Setup</h2>
    <ol>
        <li>Download a GGUF model compatible with Llama.cpp (e.g. Mistral, Llama2).</li>
        <li>Place the model file inside:</li>
    </ol>
    <pre>
models/
    </pre>
</div>

<div class="section">
    <h2>â–¶ï¸ Running the Application</h2>
    <pre>
streamlit run rag_chatbot_app.py
    </pre>

    <h3>Optional Parameters</h3>
    <pre>
streamlit run rag_chatbot_app.py -- \
  --model mistral \
  --k 3 \
  --max-new-tokens 512 \
  --chunk-size 1000 \
  --chunk-overlap 50
    </pre>
</div>

<div class="section">
    <h2>ğŸ“„ Document Upload</h2>
    <ul>
        <li>Supported format: Markdown (.md)</li>
        <li>Upload via sidebar</li>
        <li>Documents are chunked, embedded, and stored in ChromaDB</li>
        <li>Used for context-aware question answering</li>
    </ul>
</div>

<div class="section">
    <h2>ğŸ§ª Example Workflow</h2>
    <ol>
        <li>Upload Markdown documents</li>
        <li>Ask a question related to the documents</li>
        <li>Relevant chunks are retrieved</li>
        <li>LLM generates an answer using retrieved context</li>
        <li>Chat history is preserved</li>
    </ol>
</div>

<div class="section">
    <h2>ğŸ› ï¸ Configuration Parameters</h2>
    <table border="1" cellpadding="8">
        <tr>
            <th>Parameter</th>
            <th>Description</th>
            <th>Default</th>
        </tr>
        <tr>
            <td>--model</td>
            <td>LLM model name</td>
            <td>first available</td>
        </tr>
        <tr>
            <td>--k</td>
            <td>Number of retrieved chunks</td>
            <td>2</td>
        </tr>
        <tr>
            <td>--max-new-tokens</td>
            <td>Maximum tokens generated</td>
            <td>512</td>
        </tr>
        <tr>
            <td>--chunk-size</td>
            <td>Document chunk size</td>
            <td>1000</td>
        </tr>
        <tr>
            <td>--chunk-overlap</td>
            <td>Overlap between chunks</td>
            <td>50</td>
        </tr>
    </table>
</div>

<div class="section">
    <h2>ğŸ“Œ Notes</h2>
    <ul>
        <li>Vector store is persistent under <code>vector_store/docs_index/</code></li>
        <li>Chat history is session-based</li>
        <li>No external API required (fully local)</li>
    </ul>
</div>



<div class="section">
    <h2>ğŸ“œ License</h2>
    <p>This project is licensed under the MIT License.</p>
</div>



<div class="footer">
    <p>Â© 2026 Rajesh Vhankade. All rights reserved.</p>
</div>

</body>
</html>
